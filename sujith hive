[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202307201503_415176270.txt
hive> select * from customer INNER JOIN purchase ON customer.userID =purchase.userID;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201453_0002, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201453_0002
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201453_0002
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 15:04:07,404 Stage-1 map = 0%,  reduce = 0%
2023-07-20 15:04:16,634 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:17,673 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:18,750 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:19,810 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:20,860 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:21,891 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:22,921 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:23,965 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:25,075 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:26,097 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:27,123 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:28,166 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.61 sec
2023-07-20 15:04:29,349 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.61 sec
2023-07-20 15:04:30,475 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.61 sec
2023-07-20 15:04:31,496 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:32,530 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:33,563 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:34,600 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:35,643 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:36,682 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:37,761 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:38,832 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:39,919 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:41,063 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:42,077 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
2023-07-20 15:04:43,155 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.78 sec
MapReduce Total cumulative CPU time: 12 seconds 780 msec
Ended Job = job_202307201453_0002
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 12.78 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 780 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:05:00.0	100
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:08:00.0	150
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:09:00.0	200
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:13:00.0	120
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:17:00.0	80
Time taken: 60.31 seconds
hive> select * from customer INNER JOIN Clickstream ON customer.userID =Clickstream.userID;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201453_0003, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201453_0003
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201453_0003
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 15:06:01,555 Stage-1 map = 0%,  reduce = 0%
2023-07-20 15:06:10,770 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:11,784 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:12,802 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:13,824 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:14,859 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:15,910 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:16,944 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:17,987 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:19,018 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec
2023-07-20 15:06:20,051 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
2023-07-20 15:06:21,068 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
2023-07-20 15:06:22,091 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
2023-07-20 15:06:23,113 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
2023-07-20 15:06:24,137 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
2023-07-20 15:06:25,188 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
2023-07-20 15:06:26,298 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.07 sec
MapReduce Total cumulative CPU time: 10 seconds 70 msec
Ended Job = job_202307201453_0003
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 10.07 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 70 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:01:00.0	product_page
1	John Doe	john.doe@example.com	1	2023-01-01 10:00:00.0	homepage
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:02:00.0	homepage
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:03:00.0	cart_page
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:07:00.0	cart_page
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:05:00.0	homepage
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:06:00.0	product_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:09:00.0	homepage
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:10:00.0	product_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:11:00.0	cart_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:12:00.0	checkout_page
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:15:00.0	homepage
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:16:00.0	product_page
Time taken: 33.624 seconds
hive> select * from customer WHERE amount > 100;                                           
FAILED: SemanticException [Error 10004]: Line 1:29 Invalid table alias or column reference 'amount': (possible column names are: userid, name, email)
hive> select * from Commerce.customer WHERE amount > 100;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'customer'
hive> [training@localhosPrint conversation Print   Open conversation in new window New window 
bash: Print: command not found
[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202307201541_2010241704.txt
hive> select * from customer INNER JOIN Clickstream ON customer.userID =Clickstream.userID;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201453_0004, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201453_0004
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201453_0004
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 15:41:51,706 Stage-1 map = 0%,  reduce = 0%
2023-07-20 15:41:57,806 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:41:58,834 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:41:59,859 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:00,870 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:01,891 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:02,919 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:03,953 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:04,978 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:05,997 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.64 sec
2023-07-20 15:42:07,013 Stage-1 map = 100%,  reduce = 17%, Cumulative CPU 3.21 sec
2023-07-20 15:42:08,024 Stage-1 map = 100%,  reduce = 17%, Cumulative CPU 3.21 sec
2023-07-20 15:42:09,047 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.02 sec
2023-07-20 15:42:10,066 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.02 sec
2023-07-20 15:42:11,085 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.02 sec
2023-07-20 15:42:12,100 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.02 sec
2023-07-20 15:42:13,132 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.02 sec
MapReduce Total cumulative CPU time: 6 seconds 20 msec
Ended Job = job_202307201453_0004
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 6.02 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 20 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:01:00.0	product_page
1	John Doe	john.doe@example.com	1	2023-01-01 10:00:00.0	homepage
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:02:00.0	homepage
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:03:00.0	cart_page
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:07:00.0	cart_page
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:05:00.0	homepage
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:06:00.0	product_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:09:00.0	homepage
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:10:00.0	product_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:11:00.0	cart_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:12:00.0	checkout_page
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:15:00.0	homepage
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:16:00.0	product_page
Time taken: 33.001 seconds
hive> select * from Clickstream INNER JOIN purchase ON Clickstream.userID =purchase.userID;   
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201453_0005, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201453_0005
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201453_0005
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 15:48:29,478 Stage-1 map = 0%,  reduce = 0%
2023-07-20 15:48:39,610 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.67 sec
2023-07-20 15:48:40,633 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.67 sec
2023-07-20 15:48:41,644 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.67 sec
2023-07-20 15:48:42,655 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.67 sec
2023-07-20 15:48:43,678 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.67 sec
2023-07-20 15:48:44,697 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:45,707 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:46,716 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:47,727 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:48,737 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:49,755 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:50,782 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:51,803 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:52,838 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:53,852 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:54,867 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:55,891 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:56,913 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:57,924 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
2023-07-20 15:48:58,938 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.28 sec
MapReduce Total cumulative CPU time: 6 seconds 280 msec
Ended Job = job_202307201453_0005
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 6.28 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 280 msec
OK
1	2023-01-01 10:01:00.0	product_page	1	2023-01-01 10:05:00.0	100
1	2023-01-01 10:00:00.0	homepage	1	2023-01-01 10:05:00.0	100
2	2023-01-01 10:02:00.0	homepage	2	2023-01-01 10:08:00.0	150
2	2023-01-01 10:03:00.0	cart_page	2	2023-01-01 10:08:00.0	150
3	2023-01-01 10:07:00.0	cart_page	3	2023-01-01 10:09:00.0	200
3	2023-01-01 10:05:00.0	homepage	3	2023-01-01 10:09:00.0	200
3	2023-01-01 10:06:00.0	product_page	3	2023-01-01 10:09:00.0	200
4	2023-01-01 10:09:00.0	homepage	4	2023-01-01 10:13:00.0	120
4	2023-01-01 10:10:00.0	product_page	4	2023-01-01 10:13:00.0	120
4	2023-01-01 10:11:00.0	cart_page	4	2023-01-01 10:13:00.0	120
4	2023-01-01 10:12:00.0	checkout_page	4	2023-01-01 10:13:00.0	120
5	2023-01-01 10:15:00.0	homepage	5	2023-01-01 10:17:00.0	80
5	2023-01-01 10:16:00.0	product_page	5	2023-01-01 10:17:00.0	80
Time taken: 34.263 seconds
hive> 
